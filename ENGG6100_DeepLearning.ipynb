{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal Python librar\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Differentiable programming, Deep Learning frameworks/libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Kernel Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    \n",
    "    def __init__(self, x, nb_filters, use_batch_norm, phase):\n",
    "        self.__dict__.update(locals())\n",
    "        self.convolutional_layers(use_batch_norm, phase)\n",
    "    \n",
    "    def initialize_kernel(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=(0, 1, 2)))\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def initialize_weight(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=0, keep_dims=True))\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def bias_variable(self, shape):\n",
    "        init = tf.constant(0, shape=shape)\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    def convolutional_layers(self, use_batch_norm, phase):\n",
    "\n",
    "        pad_k = tf.constant([[1, 1], [1, 1], [0, 0]])\n",
    "        pad_a = tf.constant([[0, 0], [1, 1], [1, 1]])\n",
    "\n",
    "        with tf.name_scope('conv_1') as scope:\n",
    "\n",
    "            k_sz = 8 # the square kernel size\n",
    "            W_conv1 = self.initialize_kernel([k_sz, k_sz, int(self.x.shape[3]), self.nb_filters])\n",
    "            \n",
    "            '''\n",
    "            for logging to Tensorboard \"Images\" tab. \n",
    "            This is a bit of a hack to combine all 8x8 convolution\n",
    "            filters into one image.\n",
    "            '''\n",
    "            W1_c = tf.split(W_conv1, self.nb_filters, 3)  # nb_filters x [8, 8, 3, 1]\n",
    "            for i in range(self.nb_filters):\n",
    "                W1_c[i] = tf.pad(tf.reshape(\n",
    "                    W1_c[i], [k_sz, k_sz, int(self.x.shape[3])]), pad_k, \"CONSTANT\")\n",
    "            W1_row0 = tf.concat(W1_c[0:8], 0)      # [80, 10, 3, 1]\n",
    "            W1_row1 = tf.concat(W1_c[8:16], 0)     # [80, 10, 3, 1]\n",
    "            W1_row2 = tf.concat(W1_c[16:24], 0)    # [80, 10, 3, 1]\n",
    "            W1_row3 = tf.concat(W1_c[24:32], 0)    # [80, 10, 3, 1]\n",
    "            W1_row4 = tf.concat(W1_c[32:40], 0)    # [80, 10, 3, 1]\n",
    "            W1_row5 = tf.concat(W1_c[40:48], 0)    # [80, 10, 3, 1]\n",
    "            W1_row6 = tf.concat(W1_c[48:56], 0)    # [80, 10, 3, 1]\n",
    "            W1_row7 = tf.concat(W1_c[56:64], 0)    # [80, 10, 3, 1]\n",
    "            W1_d = tf.concat([W1_row0, W1_row1, W1_row2, W1_row3, W1_row4,\n",
    "                              W1_row5, W1_row6, W1_row7], 1)  # [80, 80, 3, 1]\n",
    "            W1_e = tf.reshape(W1_d, [1, 80, 80, int(self.x.shape[3])])\n",
    "            \n",
    "            # Create the actual summary to appear in Tensorboard\n",
    "            tf.summary.image(\"k1\", W1_e, 1)\n",
    "\n",
    "            # Create tensors for L1 and L2 weight decay\n",
    "            self.W_conv1_p = tf.nn.l2_loss(W_conv1)\n",
    "            self.W_conv1_l1 = tf.reduce_mean(tf.abs(W_conv1))\n",
    "            \n",
    "            h_conv1 = tf.nn.relu(tf.nn.conv2d(\n",
    "                self.x, W_conv1, strides=[1, 2, 2, 1], padding='SAME'))\n",
    "\n",
    "            a1_u, a1_var = tf.nn.moments(\n",
    "                tf.abs(h_conv1), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='h_conv1_summ', values=h_conv1)\n",
    "            tf.summary.histogram(name='W_conv1_summ', values=W_conv1)\n",
    "            tf.summary.scalar(\"conv1_activation_mean\", tf.reduce_mean(a1_u))\n",
    "            tf.summary.scalar(\"conv1_activations_var\", tf.reduce_mean(a1_var))\n",
    "\n",
    "        with tf.name_scope('conv_2') as scope:\n",
    "\n",
    "            k_sz = 6 # the square kernel size\n",
    "            in_ch = self.nb_filters\n",
    "            out_ch = self.nb_filters * 2\n",
    "            self.W_conv2 = self.initialize_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            \n",
    "            # Create tensors for L1 and L2 weight decay\n",
    "            self.W_conv2_p = tf.nn.l2_loss(self.W_conv2)\n",
    "            self.W_conv2_l1 = tf.reduce_mean(tf.abs(self.W_conv2))\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                # see https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "                # for full list of default settings.\n",
    "                h_conv1 = tf.contrib.layers.batch_norm(h_conv1, is_training=phase)\n",
    "\n",
    "            W2_c = tf.split(self.W_conv2, self.nb_filters * 2,\n",
    "                            3)  # f_out x [6, 6, f_in, 1]\n",
    "            for i in range(self.nb_filters):\n",
    "                W2_c[i] = tf.pad(tf.reshape(\n",
    "                    W2_c[i], [k_sz, k_sz, self.nb_filters]), pad_k, \"CONSTANT\")\n",
    "            W2_row0 = tf.concat(W2_c[0:8], 0)      # [64, 8, f_in, 1]\n",
    "            W2_row1 = tf.concat(W2_c[8:16], 0)     # [64, 8, f_in, 1]\n",
    "            W2_row2 = tf.concat(W2_c[16:24], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row3 = tf.concat(W2_c[24:32], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row4 = tf.concat(W2_c[32:40], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row5 = tf.concat(W2_c[40:48], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row6 = tf.concat(W2_c[48:56], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row7 = tf.concat(W2_c[56:64], 0)    # [64, 8, f_in, 1]\n",
    "            W2_d = tf.concat([W2_row0, W2_row1, W2_row2, W2_row3, W2_row4,\n",
    "                              W2_row5, W2_row6, W2_row7], 1)  # [64, 64, 3, 1]\n",
    "            W2_e = tf.reshape(W2_d, [1, 64, 64, self.nb_filters])\n",
    "            W2_f = tf.split(W2_e, self.nb_filters, 3)  # 64 x [1, 64, 64, 1]\n",
    "            W2_g = tf.concat(W2_f[0:self.nb_filters], 0)\n",
    "            \n",
    "            # Create the summary to appear in Tensorboard\n",
    "            tf.summary.image(\"k2\", W2_g, 4)\n",
    "\n",
    "            h_conv2 = tf.nn.relu(tf.nn.conv2d(\n",
    "                h_conv1, self.W_conv2, strides=[1, 2, 2, 1], padding='VALID'))\n",
    "\n",
    "            a2_u, a2_var = tf.nn.moments(\n",
    "                tf.abs(h_conv2), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='W_conv2_summ', values=self.W_conv2)\n",
    "            tf.summary.histogram(name='h_conv2_summ', values=h_conv2)\n",
    "            tf.summary.scalar(\"conv2_activation_mean\", tf.reduce_mean(a2_u))\n",
    "            tf.summary.scalar(\"conv2_activation_var\", tf.reduce_mean(a2_var))\n",
    "            \n",
    "        with tf.name_scope('conv_3') as scope:\n",
    "        \n",
    "            k_sz = 5 # the square kernel size\n",
    "            in_ch = self.nb_filters * 2\n",
    "            out_ch = self.nb_filters * 2\n",
    "            self.W_conv3 = self.initialize_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            \n",
    "            # weight decay\n",
    "            self.W_conv3_p = tf.nn.l2_loss(self.W_conv3)\n",
    "            self.W_conv3_l1 = tf.reduce_mean(tf.abs(self.W_conv3))\n",
    "    \n",
    "            if self.use_batch_norm:\n",
    "                h_conv2 = tf.contrib.layers.batch_norm(\n",
    "                    h_conv2, epsilon=BN_EPSILON, is_training=phase)\n",
    "\n",
    "            h_conv3 = tf.nn.relu(tf.nn.conv2d(\n",
    "                h_conv2, self.W_conv3, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "\n",
    "            a3_u, a3_var = tf.nn.moments(\n",
    "                tf.abs(h_conv3), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='W_conv3_summ', values=self.W_conv3)\n",
    "            tf.summary.histogram(name='h_conv3_summ', values=h_conv3)\n",
    "            tf.summary.scalar(\"conv3_activation_mean\", tf.reduce_mean(a3_u))\n",
    "            tf.summary.scalar(\"conv3_activation_var\", tf.reduce_mean(a3_var))\n",
    "            \n",
    "        with tf.name_scope('fc_out') as scope:\n",
    "\n",
    "            nb_classes = 10\n",
    "            in_sz = self.nb_filters * 8\n",
    "            W_fcout = self.initialize_weight([in_sz, nb_classes])\n",
    "            \n",
    "            # weight decay\n",
    "            self.W_fcout_p = tf.nn.l2_loss(W_fcout)\n",
    "            self.W_fcout_l1 = tf.reduce_mean(tf.abs(W_fcout))\n",
    "\n",
    "            h_conv3_flat = tf.reshape(h_conv3, [-1, in_sz])\n",
    "            \n",
    "            # tensor equivalent of numpy.dot()\n",
    "            self.output = tf.matmul(h_conv3_flat, W_fcout) \n",
    "\n",
    "            y_u, y_var = tf.nn.moments(\n",
    "                tf.abs(self.output), axes=[0], keep_dims=False)\n",
    "\n",
    "            norm_out = tf.norm(W_fcout)\n",
    "            \n",
    "            tf.summary.histogram(name='output_summ', values=self.output)\n",
    "            tf.summary.scalar(\"norm_out\", norm_out)\n",
    "            tf.summary.scalar(\"logits_mean\", tf.reduce_mean(y_u))\n",
    "            tf.summary.scalar(\"logits_var\", tf.reduce_mean(y_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for loading and preprocessing the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_cifar10():\n",
    "    \"\"\"\n",
    "    Preprocess CIFAR-10 dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # These values are specific to CIFAR-10\n",
    "    img_rows = 32\n",
    "    img_cols = 32\n",
    "    nb_classes = 10\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = data_cifar10()\n",
    "__, img_rows, img_cols, channels = train_x.shape\n",
    "__, nb_classes = train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some variables and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to keep track of how many steps we've trained our model for\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "dtype = tf.float32\n",
    "x_ = tf.placeholder(dtype, shape=(None, img_rows, img_cols, channels))\n",
    "y_ = tf.placeholder(dtype, shape=(None, nb_classes))\n",
    "\n",
    "# This is for batch normalization. True means training mode, False means testing mode.\n",
    "phase = tf.placeholder_with_default(True, shape=(), name='phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_save_path(root_path, batch_size, nb_filters, learning_rate, epochs):\n",
    "    \n",
    "    model_path = os.path.join(root_path, 'k_' + str(nb_filters))\n",
    "    model_path = os.path.join(model_path, 'bs_' + str(batch_size))\n",
    "    model_path = os.path.join(model_path, 'lr_%1.e' % learning_rate)\n",
    "    model_path = os.path.join(model_path, 'ep_' + str(epochs))\n",
    "    '''\n",
    "    optionally create this folder if it does not already exist,\n",
    "    otherwise, increment the subfolder number\n",
    "    '''\n",
    "    model_path = create_dir_if_not_exists(model_path)\n",
    "\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        path += '/1'\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        digits = []\n",
    "        sub_dirs = next(os.walk(path))[1]\n",
    "        [digits.append(s) for s in sub_dirs if s.isnumeric()]\n",
    "        if len(digits) > 0:\n",
    "            sub = str(np.max(np.asarray(sub_dirs).astype('uint8')) + 1)        \n",
    "        else:\n",
    "            sub = '1'\n",
    "        path = os.path.join(path, sub)\n",
    "        print(path)\n",
    "        os.makedirs(path)\n",
    "    print('Logging to:%s' % path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def batch_indices(batch_nb, data_length, batch_size):\n",
    "    \"\"\"\n",
    "    This helper function computes a batch start and end index\n",
    "    :param batch_nb: the batch number\n",
    "    :param data_length: the total length of the data being parsed by batches\n",
    "    :param batch_size: the number of inputs in each batch\n",
    "    :return: pair of (start, end) indices\n",
    "    \"\"\"\n",
    "    # Batch start and end index\n",
    "    start = int(batch_nb * batch_size)\n",
    "    end = int((batch_nb + 1) * batch_size)\n",
    "\n",
    "    # When there are not enough inputs left, we reuse some to complete the\n",
    "    # batch\n",
    "    if end > data_length:\n",
    "        shift = end - data_length\n",
    "        start -= shift\n",
    "        end -= shift\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_reg = 1e-2\n",
    "l1_reg = 1e-3\n",
    "nb_epochs = 5\n",
    "nb_filters = 64\n",
    "batch_size = 16 # normally use 128\n",
    "learning_rate = 1e-3\n",
    "batch_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ssd/logs/cifar10/k_64/bs_16/lr_1e-03/ep_5/15\n",
      "Logging to:/scratch/ssd/logs/cifar10/k_64/bs_16/lr_1e-03/ep_5/15\n",
      "/scratch/ssd/logs/cifar10/k_64/bs_16/lr_1e-03/ep_5/15\n"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/ssd/logs/cifar10'\n",
    "\n",
    "# assume we're going to train from scratch, and not log checkpoints\n",
    "save = False\n",
    "train_from_scratch = True\n",
    "\n",
    "if model_path is not None:\n",
    "    if os.path.exists(model_path):\n",
    "        # check for existing model in immediate subfolder\n",
    "        if any(f.endswith('.meta') for f in os.listdir(model_path)):\n",
    "            train_from_scratch = False\n",
    "        else:\n",
    "            save = True\n",
    "            model_path = build_model_save_path(\n",
    "                model_path, batch_size, nb_filters, learning_rate, nb_epochs)\n",
    "            print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = ConvNet(x_, nb_filters, batch_norm, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = cnn.output\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=y_, logits=logits))\n",
    "\n",
    "# if you just want the model predictions, use the following:\n",
    "# preds = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add weight decay to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loss += l2_reg * (cnn.W_conv1_p + cnn.W_conv2_p +\n",
    "                        cnn.W_conv3_p + cnn.W_fcout_p)\n",
    "\n",
    "total_loss += l1_reg * (cnn.W_conv1_l1 + cnn.W_conv2_l1 +\n",
    "                        cnn.W_conv3_l1 + cnn.W_fcout_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if batch_norm:\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        # ensures that we execute the update_ops before performing the train_op\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "                    total_loss, global_step, learning_rate=learning_rate, optimizer='Adam',  # SGD\n",
    "                    summaries=[\"gradients\"])\n",
    "else:\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "                total_loss, global_step, learning_rate=learning_rate, optimizer='Adam',\n",
    "                summaries=[\"gradients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup time was 0.0675\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session()\n",
    "start_time = time.time()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "d = float(time.time() - start_time)\n",
    "print(\"Startup time was %.4f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup summary writer\n",
    "if save:\n",
    "    summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "    tf.summary.scalar(\n",
    "                \"stats/training_loss\", total_loss)\n",
    "    tf.summary.scalar(\"stats/test_accuracy\", accuracy)\n",
    "    \n",
    "    # create one op that will run all summaries\n",
    "    merge_op = tf.summary.merge_all()\n",
    "    checkpoint_path = os.path.join(model_path, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, tensor, x, y, x_np, y_np, feed=None):\n",
    "    feed_dict = {x: x_np, y: y_np, phase: False}\n",
    "    if feed is not None:\n",
    "        feed_dict.update(feed)\n",
    "    return sess.run(tensor, feed_dict)\n",
    "\n",
    "def evaluate_model(sess, accuracy, x, y, test_x, test_y, batch_size):\n",
    "    \"\"\"\n",
    "    This helper function evaluates a model on one pass through\n",
    "    the test set\n",
    "    :param accuracy: the tensor that computes accuracy\n",
    "    :param x: input placeholder\n",
    "    :param y: output placeholder\n",
    "    :param test_x: the test examples\n",
    "    :param test_y: the test labels\n",
    "    :param batch_size: batch size to use when evaluating\n",
    "    :return: accuracy on the test set\n",
    "    \"\"\"\n",
    "    nb_test_examples = test_x.shape[0]\n",
    "    nb_test_batches = int(\n",
    "        np.ceil(float(nb_test_examples) / batch_size))\n",
    "    # print('nb_test_batches=%d' % nb_test_batches)\n",
    "    assert nb_test_batches * batch_size >= nb_test_examples\n",
    "\n",
    "    tot_accuracy = 0.0\n",
    "    for e, test_batch in enumerate(range(nb_test_batches)):\n",
    "        # Must not use the `batch_indices` function here, because it\n",
    "        # repeats some examples.\n",
    "        # It's acceptable to repeat during training, but not eval.\n",
    "        start = test_batch * batch_size\n",
    "        end = min(nb_test_examples, start + batch_size)\n",
    "        cur_batch_size = end - start\n",
    "        batch_xs = test_x[start:end]\n",
    "        batch_ys = test_y[start:end]\n",
    "        cur_acc = evaluate(sess, accuracy, x,\n",
    "                           y_, batch_xs, batch_ys)\n",
    "        tot_accuracy += (cur_batch_size * cur_acc)\n",
    "    tot_accuracy /= nb_test_examples\n",
    "    return tot_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/312 [00:00<00:21, 14.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_training_batches=3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:19<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss=1.6341, test_acc=0.4539 (0.8 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:19<00:00, 16.42it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss=1.6246, test_acc=0.4526 (0.8 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:19<00:00, 16.18it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss=1.9285, test_acc=0.4455 (0.8 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:19<00:00, 16.29it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss=1.6720, test_acc=0.4422 (0.8 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:19<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss=1.5987, test_acc=0.4598 (0.8 ex/s)\n"
     ]
    }
   ],
   "source": [
    "if train_from_scratch:\n",
    "    step = 0\n",
    "    init_step = 0\n",
    "    max_acc = 0\n",
    "    \n",
    "    # Compute number of training batches\n",
    "    nb_training_examples = train_x.shape[0]\n",
    "    nb_batches = int(\n",
    "    np.ceil(float(nb_training_examples) / batch_size))\n",
    "    print('nb_training_batches=%d' % nb_batches)\n",
    "    assert nb_batches * batch_size >= nb_training_examples\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        # Indices to shuffle training set\n",
    "        index_shuf = np.arange(nb_training_examples)\n",
    "        np.random.shuffle(index_shuf)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch in tqdm(range(nb_batches // 10)):\n",
    "\n",
    "            step = init_step + (epoch * nb_batches + batch)\n",
    "\n",
    "            # Compute batch start and end indices\n",
    "            start, end = batch_indices(\n",
    "                batch, nb_training_examples, batch_size)\n",
    "\n",
    "            batch_xs = train_x[index_shuf[start:end]]\n",
    "            batch_ys = train_y[index_shuf[start:end]]\n",
    "\n",
    "            __, loss_val = sess.run([train_op, total_loss], feed_dict={\n",
    "                x_: batch_xs, y_: batch_ys})\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "        # Init result var\n",
    "        tot_accuracy = evaluate_model(\n",
    "            sess, accuracy, x_, y_, test_x, test_y, batch_size)\n",
    "\n",
    "        print(\"epoch %d, loss=%.4f, test_acc=%.4f (%.1f ex/s)\" %\n",
    "              (epoch, loss_val, tot_accuracy, float(batch_size / duration)))\n",
    "\n",
    "        if model_path:\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "            __, merged_summ = sess.run([accuracy, merge_op], feed_dict={\n",
    "                x_: batch_xs, y_: batch_ys, phase: False})\n",
    "            summary_writer.add_summary(merged_summ, step)\n",
    "            summary_writer.flush()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
