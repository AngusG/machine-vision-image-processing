{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Python libraries\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable programming, Deep Learning frameworks/libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Kernel Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    \n",
    "    def __init__(self, x, nb_filters, use_batch_norm, phase, reuse):\n",
    "        self.__dict__.update(locals())\n",
    "        self.convolutional_layers(use_batch_norm, phase)\n",
    "        # calculate the number of parameters to be learned in our model\n",
    "        self.nb_params = np.sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "        print(\"Created instance of CNN model with %d parameters\" % self.nb_params)\n",
    "    \n",
    "    def initialize_kernel(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=(0, 1, 2)))\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def get_kernel(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=(0, 1, 2)))\n",
    "        return tf.get_variable(\"k\", initializer=init)\n",
    "    \n",
    "    def initialize_weight(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=0, keep_dims=True))\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def get_weight(self, shape, stddev=0.1):\n",
    "        init = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        init = init / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init), axis=0, keep_dims=True))\n",
    "        return tf.get_variable(\"w\", initializer=init)\n",
    "    \n",
    "    def bias_variable(self, shape):\n",
    "        init = tf.constant(0, shape=shape)\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    def convolutional_layers(self, use_batch_norm, phase):\n",
    "\n",
    "        pad_k = tf.constant([[1, 1], [1, 1], [0, 0]])\n",
    "        pad_a = tf.constant([[0, 0], [1, 1], [1, 1]])\n",
    "\n",
    "        with tf.name_scope('conv_1') as scope:\n",
    "\n",
    "            k_sz = 8 # the square kernel size (e.g 8x8)\n",
    "            \n",
    "            #W_conv1 = self.initialize_kernel([k_sz, k_sz, int(self.x.shape[3]), self.nb_filters])\n",
    "            '''\n",
    "            Try un-commmenting the above line in all layers and observing the number\n",
    "            of parameters change everytime you run the below cell titled\n",
    "            \"Instantiate the CNN model\". If we don't explicitly tell TensorFlow to re-use \n",
    "            variables, additional copies of the graph will be created, blowing up the \n",
    "            number of parameters. Reusing variables is accomplished by creating them under\n",
    "            a tf.variable_scope, and using tf.get_variable rather than tf.Variable.\n",
    "            If you want to force a new variable to be created everytime, you can\n",
    "            set reuse=False.\n",
    "            '''\n",
    "            with tf.variable_scope('conv_1_init', reuse=self.reuse):\n",
    "                W_conv1 = self.get_kernel([k_sz, k_sz, int(self.x.shape[3]), self.nb_filters])\n",
    "                \n",
    "            '''\n",
    "            for logging to Tensorboard \"Images\" tab. \n",
    "            This is a bit of a hack to combine all 8x8 convolution\n",
    "            filters into one image.\n",
    "            '''\n",
    "            W1_c = tf.split(W_conv1, self.nb_filters, 3)  # nb_filters x [8, 8, 3, 1]\n",
    "            for i in range(self.nb_filters):\n",
    "                W1_c[i] = tf.pad(tf.reshape(\n",
    "                    W1_c[i], [k_sz, k_sz, int(self.x.shape[3])]), pad_k, \"CONSTANT\")\n",
    "            W1_row0 = tf.concat(W1_c[0:8], 0)      # [80, 10, 3, 1]\n",
    "            W1_row1 = tf.concat(W1_c[8:16], 0)     # [80, 10, 3, 1]\n",
    "            W1_row2 = tf.concat(W1_c[16:24], 0)    # [80, 10, 3, 1]\n",
    "            W1_row3 = tf.concat(W1_c[24:32], 0)    # [80, 10, 3, 1]\n",
    "            W1_row4 = tf.concat(W1_c[32:40], 0)    # [80, 10, 3, 1]\n",
    "            W1_row5 = tf.concat(W1_c[40:48], 0)    # [80, 10, 3, 1]\n",
    "            W1_row6 = tf.concat(W1_c[48:56], 0)    # [80, 10, 3, 1]\n",
    "            W1_row7 = tf.concat(W1_c[56:64], 0)    # [80, 10, 3, 1]\n",
    "            W1_d = tf.concat([W1_row0, W1_row1, W1_row2, W1_row3, W1_row4,\n",
    "                              W1_row5, W1_row6, W1_row7], 1)  # [80, 80, 3, 1]\n",
    "            W1_e = tf.reshape(W1_d, [1, 80, 80, int(self.x.shape[3])])\n",
    "            \n",
    "            # Create the actual summary to appear in Tensorboard\n",
    "            tf.summary.image(\"k1\", W1_e, 1)\n",
    "\n",
    "            # Create tensors for L1 and L2 weight decay\n",
    "            self.W_conv1_p = tf.nn.l2_loss(W_conv1)\n",
    "            self.W_conv1_l1 = tf.reduce_mean(tf.abs(W_conv1))\n",
    "            \n",
    "            h_conv1 = tf.nn.relu(tf.nn.conv2d(\n",
    "                self.x, W_conv1, strides=[1, 2, 2, 1], padding='SAME'))\n",
    "\n",
    "            a1_u, a1_var = tf.nn.moments(\n",
    "                tf.abs(h_conv1), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='h_conv1_summ', values=h_conv1)\n",
    "            tf.summary.histogram(name='W_conv1_summ', values=W_conv1)\n",
    "            tf.summary.scalar(\"activation_mean\", tf.reduce_mean(a1_u))\n",
    "            tf.summary.scalar(\"activation_variance\", tf.reduce_mean(a1_var))\n",
    "\n",
    "        with tf.name_scope('conv_2') as scope:\n",
    "\n",
    "            k_sz = 6 # the square kernel size\n",
    "            in_ch = self.nb_filters\n",
    "            out_ch = self.nb_filters * 2\n",
    "            \n",
    "            #self.W_conv2 = self.initialize_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            with tf.variable_scope('conv_2_init', reuse=self.reuse):\n",
    "                self.W_conv2 = self.get_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            \n",
    "            # Create tensors for L1 and L2 weight decay\n",
    "            self.W_conv2_p = tf.nn.l2_loss(self.W_conv2)\n",
    "            self.W_conv2_l1 = tf.reduce_mean(tf.abs(self.W_conv2))\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                # see https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm\n",
    "                # for full list of default settings.\n",
    "                with tf.variable_scope('conv_2_bn', reuse=self.reuse):\n",
    "                    h_conv1 = tf.contrib.layers.batch_norm(h_conv1, is_training=phase)\n",
    "\n",
    "            W2_c = tf.split(self.W_conv2, self.nb_filters * 2,\n",
    "                            3)  # f_out x [6, 6, f_in, 1]\n",
    "            for i in range(self.nb_filters):\n",
    "                W2_c[i] = tf.pad(tf.reshape(\n",
    "                    W2_c[i], [k_sz, k_sz, self.nb_filters]), pad_k, \"CONSTANT\")\n",
    "            W2_row0 = tf.concat(W2_c[0:8], 0)      # [64, 8, f_in, 1]\n",
    "            W2_row1 = tf.concat(W2_c[8:16], 0)     # [64, 8, f_in, 1]\n",
    "            W2_row2 = tf.concat(W2_c[16:24], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row3 = tf.concat(W2_c[24:32], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row4 = tf.concat(W2_c[32:40], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row5 = tf.concat(W2_c[40:48], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row6 = tf.concat(W2_c[48:56], 0)    # [64, 8, f_in, 1]\n",
    "            W2_row7 = tf.concat(W2_c[56:64], 0)    # [64, 8, f_in, 1]\n",
    "            W2_d = tf.concat([W2_row0, W2_row1, W2_row2, W2_row3, W2_row4,\n",
    "                              W2_row5, W2_row6, W2_row7], 1)  # [64, 64, 3, 1]\n",
    "            W2_e = tf.reshape(W2_d, [1, 64, 64, self.nb_filters])\n",
    "            W2_f = tf.split(W2_e, self.nb_filters, 3)  # 64 x [1, 64, 64, 1]\n",
    "            W2_g = tf.concat(W2_f[0:self.nb_filters], 0)\n",
    "            \n",
    "            # Create the summary to appear in Tensorboard\n",
    "            tf.summary.image(\"k2\", W2_g, 4)\n",
    "\n",
    "            h_conv2 = tf.nn.relu(tf.nn.conv2d(\n",
    "                h_conv1, self.W_conv2, strides=[1, 2, 2, 1], padding='VALID'))\n",
    "\n",
    "            a2_u, a2_var = tf.nn.moments(\n",
    "                tf.abs(h_conv2), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='W_conv2_summ', values=self.W_conv2)\n",
    "            tf.summary.histogram(name='h_conv2_summ', values=h_conv2)\n",
    "            tf.summary.scalar(\"activation_mean\", tf.reduce_mean(a2_u))\n",
    "            tf.summary.scalar(\"activation_variance\", tf.reduce_mean(a2_var))\n",
    "            \n",
    "        with tf.name_scope('conv_3') as scope:\n",
    "        \n",
    "            k_sz = 5 # the square kernel size\n",
    "            in_ch = self.nb_filters * 2\n",
    "            out_ch = self.nb_filters * 2\n",
    "            \n",
    "            #self.W_conv3 = self.initialize_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            with tf.variable_scope('conv_3_init', reuse=self.reuse):\n",
    "                self.W_conv3 = self.get_kernel([k_sz, k_sz, in_ch, out_ch])\n",
    "            \n",
    "            # weight decay\n",
    "            self.W_conv3_p = tf.nn.l2_loss(self.W_conv3)\n",
    "            self.W_conv3_l1 = tf.reduce_mean(tf.abs(self.W_conv3))\n",
    "    \n",
    "            if self.use_batch_norm:\n",
    "                with tf.variable_scope('conv_3_bn', reuse=self.reuse):\n",
    "                    h_conv2 = tf.contrib.layers.batch_norm(h_conv2, is_training=phase)\n",
    "\n",
    "            h_conv3 = tf.nn.relu(tf.nn.conv2d(\n",
    "                h_conv2, self.W_conv3, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "\n",
    "            a3_u, a3_var = tf.nn.moments(\n",
    "                tf.abs(h_conv3), axes=[0], keep_dims=False)\n",
    "\n",
    "            tf.summary.histogram(name='W_conv3_summ', values=self.W_conv3)\n",
    "            tf.summary.histogram(name='h_conv3_summ', values=h_conv3)\n",
    "            tf.summary.scalar(\"activation_mean\", tf.reduce_mean(a3_u))\n",
    "            tf.summary.scalar(\"activation_variance\", tf.reduce_mean(a3_var))\n",
    "            \n",
    "        with tf.name_scope('fc_out') as scope:\n",
    "\n",
    "            nb_classes = 10\n",
    "            in_sz = self.nb_filters * 8\n",
    "            \n",
    "            #W_fcout = self.initialize_weight([in_sz, nb_classes])\n",
    "            with tf.variable_scope('fc_out_init', reuse=self.reuse):\n",
    "                W_fcout = self.get_weight([in_sz, nb_classes])\n",
    "            \n",
    "            # weight decay\n",
    "            self.W_fcout_p = tf.nn.l2_loss(W_fcout)\n",
    "            self.W_fcout_l1 = tf.reduce_mean(tf.abs(W_fcout))\n",
    "\n",
    "            h_conv3_flat = tf.reshape(h_conv3, [-1, in_sz])\n",
    "            \n",
    "            # tensor equivalent of numpy.dot()\n",
    "            self.output = tf.matmul(h_conv3_flat, W_fcout) \n",
    "\n",
    "            y_u, y_var = tf.nn.moments(\n",
    "                tf.abs(self.output), axes=[0], keep_dims=False)\n",
    "\n",
    "            norm_out = tf.norm(W_fcout)\n",
    "            \n",
    "            tf.summary.histogram(name='output_summ', values=self.output)\n",
    "            tf.summary.scalar(\"norm_out\", norm_out)\n",
    "            tf.summary.scalar(\"logits_mean\", tf.reduce_mean(y_u))\n",
    "            tf.summary.scalar(\"logits_var\", tf.reduce_mean(y_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for loading and preprocessing the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cifar10():\n",
    "    \"\"\"\n",
    "    Preprocess CIFAR-10 dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # These values are specific to CIFAR-10\n",
    "    img_rows = 32\n",
    "    img_cols = 32\n",
    "    nb_classes = 10\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = data_cifar10()\n",
    "__, img_rows, img_cols, channels = train_x.shape\n",
    "__, nb_classes = train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some variables and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to keep track of how many steps we've trained our model for\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "dtype = tf.float32\n",
    "x_ = tf.placeholder(dtype, shape=(None, img_rows, img_cols, channels))\n",
    "y_ = tf.placeholder(dtype, shape=(None, nb_classes))\n",
    "\n",
    "# This is for batch normalization. True means training mode, False means testing mode.\n",
    "phase = tf.placeholder_with_default(True, shape=(), name='phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_save_path(root_path, batch_size, nb_filters, learning_rate, epochs):\n",
    "    \n",
    "    model_path = os.path.join(root_path, 'k_' + str(nb_filters))\n",
    "    model_path = os.path.join(model_path, 'bs_' + str(batch_size))\n",
    "    model_path = os.path.join(model_path, 'lr_%1.e' % learning_rate)\n",
    "    model_path = os.path.join(model_path, 'ep_' + str(epochs))\n",
    "    '''\n",
    "    optionally create this folder if it does not already exist,\n",
    "    otherwise, increment the subfolder number\n",
    "    '''\n",
    "    model_path = create_dir_if_not_exists(model_path)\n",
    "\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        path += '/1'\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        digits = []\n",
    "        sub_dirs = next(os.walk(path))[1]\n",
    "        [digits.append(s) for s in sub_dirs if s.isnumeric()]\n",
    "        if len(digits) > 0:\n",
    "            sub = str(np.max(np.asarray(sub_dirs).astype('uint8')) + 1)        \n",
    "        else:\n",
    "            sub = '1'\n",
    "        path = os.path.join(path, sub)\n",
    "        os.makedirs(path)\n",
    "    print('Logging to:%s' % path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def batch_indices(batch_nb, data_length, batch_size):\n",
    "    \"\"\"\n",
    "    This helper function computes a batch start and end index\n",
    "    :param batch_nb: the batch number\n",
    "    :param data_length: the total length of the data being parsed by batches\n",
    "    :param batch_size: the number of inputs in each batch\n",
    "    :return: pair of (start, end) indices\n",
    "    \"\"\"\n",
    "    # Batch start and end index\n",
    "    start = int(batch_nb * batch_size)\n",
    "    end = int((batch_nb + 1) * batch_size)\n",
    "\n",
    "    # When there are not enough inputs left, we reuse some to complete the\n",
    "    # batch\n",
    "    if end > data_length:\n",
    "        shift = end - data_length\n",
    "        start -= shift\n",
    "        end -= shift\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = 1e-3\n",
    "l1_reg = 1e-3\n",
    "nb_epochs = 15\n",
    "nb_filters = 64\n",
    "batch_size = 128 # normally use 128\n",
    "learning_rate = 1e-3\n",
    "batch_norm = False\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to:/scratch/ssd/logs/cifar10/k_64/bs_128/lr_1e-03/ep_15/15\n"
     ]
    }
   ],
   "source": [
    "model_path = '/scratch/ssd/logs/cifar10'\n",
    "\n",
    "# assume we're going to train from scratch, and not log checkpoints\n",
    "save = False\n",
    "train_from_scratch = True\n",
    "\n",
    "if model_path is not None:\n",
    "    if os.path.exists(model_path):\n",
    "        # check for existing model in immediate subfolder\n",
    "        if any(f.endswith('.meta') for f in os.listdir(model_path)):\n",
    "            train_from_scratch = False\n",
    "        else:\n",
    "            save = True\n",
    "            model_path = build_model_save_path(\n",
    "                model_path, batch_size, nb_filters, learning_rate, nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the CNN model\n",
    "\n",
    "With the default settings and when reusing variables, we should get $721,920$ parameters \n",
    "if `batch_norm=False`, and $721,920 + 192 = 722,112$ parameters if `batch_norm=True`.\n",
    "Options for the reuse argument are `False`, `True`, and `tf.AUTO_REUSE`. The call will\n",
    "fail if you set `reuse=True` the first time you run the below cell, since the variables\n",
    "have not yet been created. The call will fail on subsequent calls if you set\n",
    "`reuse=False`, since variables with the same name already exist. This flow generally\n",
    "prevents you from making silly mistakes and enforces the desirable behaviour. `tf.AUTO_REUSE`\n",
    "will always do the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created instance of CNN model with 721920 parameters\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet(x_, nb_filters, batch_norm, phase, reuse=tf.AUTO_REUSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cnn.output\n",
    "\n",
    "total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=y_, logits=logits))\n",
    "\n",
    "# if you just want the model predictions, use the following:\n",
    "# preds = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weight decay to loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss += l2_reg * (cnn.W_conv1_p + cnn.W_conv2_p +\n",
    "                        cnn.W_conv3_p + cnn.W_fcout_p)\n",
    "\n",
    "total_loss += l1_reg * (cnn.W_conv1_l1 + cnn.W_conv2_l1 +\n",
    "                        cnn.W_conv3_l1 + cnn.W_fcout_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training op, and batch normalization if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_norm:\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        # ensures that we execute the update_ops before performing the train_op\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "                    total_loss, global_step, learning_rate=learning_rate, optimizer='Adam',  # SGD\n",
    "                    summaries=[\"gradients\"])\n",
    "else:\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "                total_loss, global_step, learning_rate=learning_rate, optimizer='Adam',\n",
    "                summaries=[\"gradients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create tensors that will automatically compute the number of \n",
    "correct predictions and accuracy in a sample\n",
    "'''\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup time was 0.1000\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session()\n",
    "start_time = time.time()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "d = float(time.time() - start_time)\n",
    "print(\"Startup time was %.4f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup summary writer\n",
    "if save:\n",
    "    summary_writer = tf.summary.FileWriter(model_path, sess.graph)\n",
    "    tf.summary.scalar(\n",
    "                \"stats/train_loss\", total_loss)\n",
    "    tf.summary.scalar(\"stats/train_accuracy\", accuracy)\n",
    "    \n",
    "    # create one op that will run all summaries\n",
    "    merge_op = tf.summary.merge_all()\n",
    "    checkpoint_path = os.path.join(model_path, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, tensor, x, y, x_np, y_np, feed=None):\n",
    "    feed_dict = {x: x_np, y: y_np, phase: False}\n",
    "    if feed is not None:\n",
    "        feed_dict.update(feed)\n",
    "    return sess.run(tensor, feed_dict)\n",
    "\n",
    "def evaluate_model(sess, accuracy, x, y, test_x, test_y, batch_size):\n",
    "    \"\"\"\n",
    "    This helper function evaluates a model on one pass through\n",
    "    the test set\n",
    "    :param accuracy: the tensor that computes accuracy\n",
    "    :param x: input placeholder\n",
    "    :param y: output placeholder\n",
    "    :param test_x: the test examples\n",
    "    :param test_y: the test labels\n",
    "    :param batch_size: batch size to use when evaluating\n",
    "    :return: accuracy on the test set\n",
    "    \"\"\"\n",
    "    nb_test_examples = test_x.shape[0]\n",
    "    nb_test_batches = int(\n",
    "        np.ceil(float(nb_test_examples) / batch_size))\n",
    "    # print('nb_test_batches=%d' % nb_test_batches)\n",
    "    assert nb_test_batches * batch_size >= nb_test_examples\n",
    "\n",
    "    tot_accuracy = 0.0\n",
    "    for e, test_batch in enumerate(range(nb_test_batches)):\n",
    "        # Must not use the `batch_indices` function here, because it\n",
    "        # repeats some examples.\n",
    "        # It's acceptable to repeat during training, but not eval.\n",
    "        start = test_batch * batch_size\n",
    "        end = min(nb_test_examples, start + batch_size)\n",
    "        cur_batch_size = end - start\n",
    "        batch_xs = test_x[start:end]\n",
    "        batch_ys = test_y[start:end]\n",
    "        cur_acc = evaluate(sess, accuracy, x,\n",
    "                           y_, batch_xs, batch_ys)\n",
    "        tot_accuracy += (cur_batch_size * cur_acc)\n",
    "    tot_accuracy /= nb_test_examples\n",
    "    return tot_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_training_batches=391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/391 [00:00<01:29,  4.36it/s]\u001b[A\n",
      "  1%|          | 2/391 [00:00<01:07,  5.80it/s]\u001b[A\n",
      "  1%|          | 4/391 [00:00<00:50,  7.65it/s]\u001b[A\n",
      "  2%|▏         | 6/391 [00:00<00:48,  7.88it/s]\u001b[A\n",
      "  2%|▏         | 8/391 [00:00<00:45,  8.37it/s]\u001b[A\n",
      "  2%|▏         | 9/391 [00:01<00:45,  8.45it/s]\u001b[A\n",
      "  3%|▎         | 10/391 [00:01<00:44,  8.51it/s]\u001b[A\n",
      "  3%|▎         | 11/391 [00:01<00:44,  8.53it/s]\u001b[A\n",
      "  3%|▎         | 13/391 [00:01<00:42,  8.90it/s]\u001b[A\n",
      "  4%|▎         | 14/391 [00:01<00:42,  8.82it/s]\u001b[A\n",
      "  4%|▍         | 16/391 [00:01<00:42,  8.82it/s]\u001b[A\n",
      "  4%|▍         | 17/391 [00:01<00:42,  8.75it/s]\u001b[A\n",
      "  5%|▍         | 18/391 [00:02<00:42,  8.70it/s]\u001b[A\n",
      "  5%|▍         | 19/391 [00:02<00:42,  8.72it/s]\u001b[A\n",
      "  5%|▌         | 20/391 [00:02<00:42,  8.71it/s]\u001b[A\n",
      "  5%|▌         | 21/391 [00:02<00:42,  8.69it/s]\u001b[A\n",
      "  6%|▌         | 22/391 [00:02<00:42,  8.72it/s]\u001b[A\n",
      "  6%|▌         | 24/391 [00:02<00:41,  8.95it/s]\u001b[A\n",
      "  6%|▋         | 25/391 [00:02<00:40,  8.98it/s]\u001b[A\n",
      "  7%|▋         | 26/391 [00:02<00:40,  8.93it/s]\u001b[A\n",
      "  7%|▋         | 27/391 [00:03<00:41,  8.85it/s]\u001b[A\n",
      "  7%|▋         | 29/391 [00:03<00:40,  9.00it/s]\u001b[A\n",
      "  8%|▊         | 30/391 [00:03<00:40,  8.94it/s]\u001b[A\n",
      "  8%|▊         | 31/391 [00:03<00:40,  8.94it/s]\u001b[A\n",
      "  8%|▊         | 32/391 [00:03<00:40,  8.95it/s]\u001b[A\n",
      "  8%|▊         | 33/391 [00:03<00:40,  8.91it/s]\u001b[A\n",
      "  9%|▊         | 34/391 [00:03<00:40,  8.92it/s]\u001b[A\n",
      "  9%|▉         | 35/391 [00:03<00:40,  8.89it/s]\u001b[A\n",
      "  9%|▉         | 36/391 [00:04<00:40,  8.87it/s]\u001b[A\n",
      " 10%|▉         | 38/391 [00:04<00:39,  8.96it/s]\u001b[A\n",
      " 10%|▉         | 39/391 [00:04<00:39,  8.94it/s]\u001b[A\n",
      " 10%|█         | 41/391 [00:04<00:38,  9.02it/s]\u001b[A\n",
      " 11%|█         | 42/391 [00:04<00:38,  9.03it/s]\u001b[A\n",
      " 11%|█         | 43/391 [00:04<00:38,  9.04it/s]\u001b[A\n",
      " 11%|█▏        | 44/391 [00:04<00:38,  9.01it/s]\u001b[A\n",
      " 12%|█▏        | 45/391 [00:05<00:38,  8.95it/s]\u001b[A\n",
      " 12%|█▏        | 47/391 [00:05<00:38,  9.03it/s]\u001b[A\n",
      " 12%|█▏        | 48/391 [00:05<00:38,  9.01it/s]\u001b[A\n",
      " 13%|█▎        | 49/391 [00:05<00:38,  8.93it/s]\u001b[A\n",
      " 13%|█▎        | 50/391 [00:05<00:38,  8.95it/s]\u001b[A\n",
      " 13%|█▎        | 51/391 [00:05<00:38,  8.89it/s]\u001b[A\n",
      " 13%|█▎        | 52/391 [00:05<00:38,  8.86it/s]\u001b[A\n",
      " 14%|█▎        | 53/391 [00:05<00:38,  8.85it/s]\u001b[A\n",
      " 14%|█▍        | 55/391 [00:06<00:37,  8.91it/s]\u001b[A\n",
      " 15%|█▍        | 57/391 [00:06<00:37,  9.00it/s]\u001b[A\n",
      " 15%|█▍        | 58/391 [00:06<00:37,  8.98it/s]\u001b[A\n",
      " 15%|█▌        | 60/391 [00:06<00:36,  9.03it/s]\u001b[A\n",
      " 16%|█▌        | 62/391 [00:06<00:36,  9.12it/s]\u001b[A\n",
      " 16%|█▋        | 64/391 [00:06<00:35,  9.18it/s]\u001b[A\n",
      " 17%|█▋        | 66/391 [00:07<00:35,  9.22it/s]\u001b[A\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/mlrg/gallowaa/anaconda2/envs/tf140-py35/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/export/mlrg/gallowaa/anaconda2/envs/tf140-py35/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/export/mlrg/gallowaa/anaconda2/envs/tf140-py35/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 391/391 [00:39<00:00,  9.90it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss=1.4422, test_acc=0.4973 (1262.1 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:33<00:00, 11.59it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss=1.4067, test_acc=0.5483 (1631.3 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss=1.2147, test_acc=0.5891 (1526.7 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 11.16it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss=1.2353, test_acc=0.6046 (1453.0 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 11.11it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss=0.9989, test_acc=0.6215 (1519.1 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.90it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss=1.0060, test_acc=0.6246 (1482.7 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 11.00it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss=1.0205, test_acc=0.6452 (1606.5 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 12.05it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss=0.9966, test_acc=0.6566 (1465.8 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 11.94it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss=0.8779, test_acc=0.6582 (1846.5 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 11.94it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss=0.8841, test_acc=0.6695 (1398.5 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:33<00:00, 11.71it/s]\n",
      "  0%|          | 1/391 [00:00<00:43,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss=0.8748, test_acc=0.6748 (1336.9 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:34<00:00, 11.34it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss=0.7788, test_acc=0.6673 (1173.9 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss=0.7979, test_acc=0.6774 (1313.5 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.57it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss=0.8270, test_acc=0.6780 (1254.1 ex/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss=0.9822, test_acc=0.6757 (1499.0 ex/s)\n"
     ]
    }
   ],
   "source": [
    "if train_from_scratch:\n",
    "    step = 0\n",
    "    init_step = 0\n",
    "    max_acc = 0\n",
    "    \n",
    "    # Compute number of training batches\n",
    "    nb_training_examples = train_x.shape[0]\n",
    "    nb_batches = int(\n",
    "    np.ceil(float(nb_training_examples) / batch_size))\n",
    "    print('nb_training_batches=%d' % nb_batches)\n",
    "    assert nb_batches * batch_size >= nb_training_examples\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        # Indices to shuffle training set\n",
    "        index_shuf = np.arange(nb_training_examples)\n",
    "        np.random.shuffle(index_shuf)\n",
    "\n",
    "        for batch in tqdm(range(nb_batches)):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            step = init_step + (epoch * nb_batches + batch)\n",
    "\n",
    "            # Compute batch start and end indices\n",
    "            start, end = batch_indices(\n",
    "                batch, nb_training_examples, batch_size)\n",
    "\n",
    "            batch_xs = train_x[index_shuf[start:end]]\n",
    "            batch_ys = train_y[index_shuf[start:end]]\n",
    "\n",
    "            __, loss_val, summ = sess.run([train_op, total_loss, merge_op], feed_dict={\n",
    "                x_: batch_xs, y_: batch_ys})\n",
    "            duration = time.time() - start_time\n",
    "            summary_writer.add_summary(summ, global_step=step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        # Init result var\n",
    "        tot_accuracy = evaluate_model(\n",
    "            sess, accuracy, x_, y_, test_x, test_y, batch_size)\n",
    "        #summary_writer.add_summary(summary, global_step=step)        \n",
    "\n",
    "        print(\"epoch %d, loss=%.4f, test_acc=%.4f (%.1f ex/s)\" %\n",
    "              (epoch, loss_val, tot_accuracy, float(batch_size / duration)))\n",
    "\n",
    "        if model_path:\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "            #__, merged_summ = sess.run([accuracy, merge_op], feed_dict={\n",
    "            #    x_: batch_xs, y_: batch_ys, phase: False})\n",
    "            #summary_writer.add_summary(merged_summ, step)\n",
    "            #summary_writer.flush()\n",
    "        step += 1\n",
    "    # close the TensorFlow client session\n",
    "    tf.reset_default_graph()\n",
    "    sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
